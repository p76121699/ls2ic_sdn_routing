{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97413dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#===================== Parameter settings =========================\n",
    "#for training\n",
    "algorithms = {\n",
    "    \"LS2IC\": \"../results/ls2ic\",\n",
    "    \"MADQN\": \"../results/ps_dqn\",\n",
    "    \"MADQN-PA\": \"../results/ps_dqn_a\",\n",
    "    \"MF-Q\": \"../results/meanfield\"\n",
    "}\n",
    "num_agents = 992 #32*31, geant topolopy is 506 = 23*22\n",
    "\n",
    "# for testing\n",
    "algorithm_dirs = {\n",
    "    \"LS2IC\": \"../results/ls2ic\",\n",
    "    \"MADQN\": \"../results/ps_dqn\",\n",
    "    \"MADQN-PA\": \"../results/ps_dqn_a\",\n",
    "    \"MF-Q\": \"../results/meanfield\",\n",
    "    \"DRSIR\": \"../results/drsir\",\n",
    "    \"OSPF\": \"../results/ospf\"\n",
    "}\n",
    "\n",
    "# testing tm id\n",
    "tm_files = [\"06_eval_metrics.csv\", \"41_eval_metrics.csv\", \"73_eval_metrics.csv\", \"108_eval_metrics.csv\", \"141_eval_metrics.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth(values, smoothing_factor):\n",
    "    smoothed = []\n",
    "    for i, val in enumerate(values):\n",
    "        if i == 0:\n",
    "            smoothed.append(val)\n",
    "        else:\n",
    "            smoothed.append(val * (1 - smoothing_factor) + smoothed[i - 1] * smoothing_factor)\n",
    "    return smoothed\n",
    "\n",
    "def load_rewards_from_file(file_path, divisor=992):\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"can't found: {file_path}\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    raw_tokens = \",\".join(lines).split(',')\n",
    "    float_rewards = []\n",
    "    for token in raw_tokens[1:-1]:\n",
    "        try:\n",
    "            val = float(token) / divisor\n",
    "            float_rewards.append(val)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return float_rewards\n",
    "\n",
    "def plot_algorithms(algorithms, smoothing_factor=0.95, divisor=992, ylabel=\"Average reward\", output_file=\"training_reward.svg\"):\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.xlabel('Step', fontsize=16)\n",
    "    plt.ylabel(ylabel, fontsize=16)\n",
    "\n",
    "    colors = plt.cm.tab10.colors \n",
    "    for idx, (label, dir_path) in enumerate(algorithms.items()):\n",
    "        file_path = os.path.join(dir_path, \"output.txt\")\n",
    "        rewards = load_rewards_from_file(file_path, divisor=divisor)\n",
    "        if not rewards:\n",
    "            continue\n",
    "        smoothed = get_smooth(rewards, smoothing_factor)\n",
    "        steps = np.arange(len(smoothed))\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.plot(steps, smoothed, alpha=0.5, color=color, label=label)\n",
    "\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.ylim(bottom=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format='svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plot_algorithms(\n",
    "        algorithms=algorithms,\n",
    "        smoothing_factor=0.95,\n",
    "        divisor=num_agents,\n",
    "        ylabel=\"Average reward\",\n",
    "        output_file=\"Training_Reward.svg\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = {alg: {} for alg in algorithm_dirs}\n",
    "\n",
    "# Read each algorithm's CSV file and calculate the average of each column\n",
    "for file in tm_files:\n",
    "    tm_id = file.split(\"_\")[0]  # Get TM ID\n",
    "    \n",
    "    for alg, dir_path in algorithm_dirs.items():\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            metrics_data[alg][tm_id] = {\n",
    "                \"avg_delay\": df[\"avg_delay\"].mean(),\n",
    "                \"avg_packet_loss\": df[\"avg_packet_loss\"].mean(),\n",
    "                \"avg_throughput\": df[\"avg_throughput\"].mean(),\n",
    "                \"max_link_utilization\": df[\"max_link_utilization\"].mean()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file} ({alg}):\", e)\n",
    "\n",
    "# Print results for verification\n",
    "for alg, data in metrics_data.items():\n",
    "    print(f\"Algorithm {alg} metrics:\")\n",
    "    print(data)\n",
    "\n",
    "def plot_all_metrics(metrics_data):\n",
    "    \"\"\"\n",
    "    Plot all four metrics for each algorithm, adaptable to different numbers of algorithms.\n",
    "    \"\"\"\n",
    "    metrics = [\"avg_delay\", \"avg_packet_loss\", \"avg_throughput\", \"max_link_utilization\"]\n",
    "    titles = [\"(a) Average Link Delay\", \"(b) Average Link Packet Loss\", \"(c) Average Link Throughput\", \"(d) Maximum Link Utilization\"]\n",
    "    ylabels = [\"Delay (ms)\", \"Packet Loss (%)\", \"Throughput (Mb/s)\", \"Utilization (%)\"]\n",
    "    \n",
    "    tm_ids = sorted(next(iter(metrics_data.values())).keys(), key=lambda x: int(x))\n",
    "    x = np.arange(len(tm_ids))  # X-axis ticks\n",
    "    width = 0.8 / len(metrics_data)  # Dynamically adjust bar width\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#FF7F00\", \"#984EA3\", \"#A65628\"]\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        for j, (alg, data) in enumerate(metrics_data.items()):\n",
    "            values = [data[tm][metric] for tm in tm_ids]\n",
    "            ax.bar(x + (j - len(metrics_data)/2 + 0.5) * width, values, width, label=alg, color=colors[j % len(colors)])\n",
    "        \n",
    "        ax.set_xlabel(titles[i], fontsize=16)\n",
    "        ax.set_ylabel(ylabels[i], fontsize=16)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(tm_ids, fontsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "    fig.legend(metrics_data.keys(), loc='upper center', bbox_to_anchor=(0.5, 1.06), ncol=len(metrics_data.keys()), fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 1])\n",
    "    plt.savefig(\"Figure 18. Testing result in 32-node 144TM.svg\", format='svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def compute_relative_improvements_no_avg(metrics_data):\n",
    "    \"\"\"\n",
    "    Compute relative improvement tables (5×5 for alg×alg) for each metric & TM\n",
    "    without averaging over TMs.\n",
    "\n",
    "    Return structure: improvements_tables[metric][tm] = pd.DataFrame(alg x alg)\n",
    "    Each cell stores the improvement percentage of A compared to B.\n",
    "    \"\"\"\n",
    "    # Define which metrics are \"lower is better\" or \"higher is better\"\n",
    "    metric_directions = {\n",
    "        'avg_delay': 'lower',\n",
    "        'avg_packet_loss': 'lower',\n",
    "        'avg_throughput': 'higher',\n",
    "        'max_link_utilization': 'lower'\n",
    "    }\n",
    "\n",
    "    metrics = list(metric_directions.keys())        # Four metrics\n",
    "    alg_list = list(metrics_data.keys())            # Five algorithms\n",
    "    \n",
    "    # Get TM IDs from the first algorithm (e.g., 06, 41, 73, 108, 141)\n",
    "    example_alg = alg_list[0]\n",
    "    tm_ids = sorted(metrics_data[example_alg].keys(), key=lambda x: int(x))\n",
    "\n",
    "    # Final structure to store all tables: {metric -> {tm -> df(5×5)}}\n",
    "    improvement_tables = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        direction = metric_directions[metric]\n",
    "        improvement_tables[metric] = {}  # Per-TM tables for this metric\n",
    "\n",
    "        for tm in tm_ids:\n",
    "            # Create a 5×5 table for this metric & TM (row: algA, col: algB)\n",
    "            df_improvement = pd.DataFrame(index=alg_list, columns=alg_list, dtype=float)\n",
    "\n",
    "            for algA in alg_list:\n",
    "                for algB in alg_list:\n",
    "                    if algA == algB:\n",
    "                        df_improvement.loc[algA, algB] = np.nan\n",
    "                        continue\n",
    "                    \n",
    "                    valA = metrics_data[algA][tm][metric]\n",
    "                    valB = metrics_data[algB][tm][metric]\n",
    "\n",
    "                    # Avoid division by zero\n",
    "                    if valB == 0:\n",
    "                        improvement = np.nan\n",
    "                    else:\n",
    "                        # lower: (valB - valA)/valB * 100  (lower A is better)\n",
    "                        # higher: (valA - valB)/valB * 100 (higher A is better)\n",
    "                        if direction == 'lower':\n",
    "                            improvement = (valB - valA) / valB * 100.0\n",
    "                        else:\n",
    "                            improvement = (valA - valB) / valB * 100.0\n",
    "\n",
    "                    df_improvement.loc[algA, algB] = improvement\n",
    "            \n",
    "            # Store the table\n",
    "            improvement_tables[metric][tm] = df_improvement\n",
    "\n",
    "    return improvement_tables\n",
    "\n",
    "def print_improvement_tables_no_avg(improvement_tables):\n",
    "    \"\"\"\n",
    "    Print the non-averaged relative improvement results (4×{TM->5×5}) all at once.\n",
    "    \"\"\"\n",
    "    for metric, tm_dict in improvement_tables.items():\n",
    "        print(f\"=== {metric} ===\")\n",
    "        for tm, df in tm_dict.items():\n",
    "            print(f\"--- TM = {tm} ---\")\n",
    "            print(df.to_string(float_format=lambda x: f\"{x:6.2f}%\"))\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    win_tables = compute_relative_improvements_no_avg(metrics_data)\n",
    "    print_improvement_tables_no_avg(win_tables)\n",
    "    plot_all_metrics(metrics_data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
